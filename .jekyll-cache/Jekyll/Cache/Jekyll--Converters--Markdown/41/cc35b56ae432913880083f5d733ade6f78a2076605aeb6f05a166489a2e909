I"¸<p><a href="https://youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv">cs231n ê°•ì˜</a></p>

<p><br /></p>

<p>LeNet-5: ì„±ê³µì ì¸ ìµœì´ˆì˜ ConvNet</p>

<ul>
  <li>stride = 1 ì¸ 5x5 filter</li>
  <li>Convì™€ Pooling Layer ê±°ì¹œ í›„ FC Layerê°€ ë¶™ìŒ</li>
  <li>ê°„ë‹¨í•œ ëª¨ë¸ì´ë‚˜, ìˆ«ì ì¸ì‹ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ìŒ</li>
</ul>

<p><br /></p>

<p>AlexNet</p>

<ul>
  <li>ìµœì´ˆì˜ large scale CNN</li>
  <li>ImageNet Classification taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
  <li>Conv-Pool-Norm êµ¬ì¡° ë‘ë²ˆ ë°˜ë³µ í›„ Conv Pool FC layerê°€ ë¶™ìŒ</li>
  <li>LeNetê³¼ ìœ ì‚¬, layerê°€ ë” ë§ì•„ì§(5ê°œ conv, 2ê°œ FC)
    <ul>
      <li>ì°¸ê³ ) Pooling layerì—ëŠ” parameterê°€ ì—†ìŒ</li>
    </ul>
  </li>
  <li>ReLU ì‚¬ìš©</li>
  <li>Norm layer ìˆìŒ(ì§€ê¸ˆì€ ì‚¬ìš© x)</li>
  <li>data augmentationì„ ë§ì´ í•¨ (flipping, jittering, color norm)</li>
  <li>dropout 0.5, batch size 128, SGD momentum 0.9, initial lr 1e-2, final lr 12-10, weight decay, model ensemble</li>
  <li>ëª¨ë¸ì´ ë‘ê°œë¡œ ë‚˜ëˆ„ì–´ì ¸ êµì°¨í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ(GPUì˜ ìš©ëŸ‰ í•œê³„ ë•Œë¬¸)
    <ul>
      <li>Conv 1,2,4,5ì—ì„œëŠ” ê°™ì€ GPU ë‚´ì˜ feature mapë§Œ ì´ìš©</li>
      <li>Conv 3, FC 6,7,8ì—ì„œëŠ” ì´ì „ input layerì˜ ì „ì²´ depthë¥¼ ì „ë¶€ ê°€ì ¸ì˜´</li>
    </ul>
  </li>
  <li>transfer learningì— ë§ì´ ì‚¬ìš©ë˜ì—ˆìŒ</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/71377968/165305125-f35ecbae-cf52-4e9f-ada5-1de520b6d372.png" alt="image" /></p>

<p><br /></p>

<p>ZFNet</p>

<ul>
  <li>AlexNetì˜ hyperparameterë¥¼ ê°œì„ í•œ ëª¨ë¸</li>
</ul>

<p><br /></p>

<p>VGGNet(ì´ ë•Œë¶€í„° ë„¤íŠ¸ì›Œí¬ê°€ í›¨ì”¬ ê¹Šì–´ì§)</p>

<ul>
  <li>ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬(16-19 layer)</li>
  <li>ë” ì‘ì€ filter(3x3)
    <ul>
      <li>depthë¥¼ ë” í‚¤ìš¸ ìˆ˜ ìˆìŒ</li>
      <li>7x7 filterì™€ ì‹¤ì§ˆì ìœ¼ë¡œ ë™ì¼í•œ receptive fieldë¥¼ ê°€ì§€ë©´ì„œë„ ë” ê¹Šì€ layerë¥¼ ìŒ“ì„ ìˆ˜ ìˆìŒ</li>
      <li>non linearity ì¶”ê°€ ê°€ëŠ¥</li>
      <li>parameter ì¤„ì–´ë“¦</li>
    </ul>
  </li>
  <li>AlexNetê³¼ ë¹„ìŠ·í•œ íŒ¨í„´ ë°˜ë³µ: Convì™€ Pooling layerê°€ ë°˜ë³µì ìœ¼ë¡œ ì§„í–‰. ë‹¨, local response normalizationì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ</li>
  <li>VGG16ê³¼ VGG19ê°€ ìˆëŠ”ë°, 19ëŠ” ì„±ëŠ¥ì´ ì•„ì£¼ ì¡°ê¸ˆ ì˜¬ë¼ê°€ì§€ë§Œ ë©”ëª¨ë¦¬ë„ ë” ì‚¬ìš©í•˜ë¯€ë¡œ ì£¼ë¡œ VGG16 ì‚¬ìš©</li>
  <li>ë§ˆì§€ë§‰ FC7ì€ 4096 ì‚¬ì´ì¦ˆì˜ ë ˆì´ì–´ë¡œ, ì•„ì£¼ ì¢‹ì€ featrue representationì„ ê°€ì§</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/71377968/165305139-085e87cd-a783-425b-8fb3-7fbcc648b4c9.png" alt="image" /></p>

<p><br /></p>

<p>GoogLeNet</p>

<ul>
  <li>22 layer</li>
  <li>FC layerëŠ” ì—†ìŒ â†’ íŒŒë¼ë¯¸í„° ê°ì†Œ</li>
  <li>Inception Module ìŒ“ì•„ ì‚¬ìš©
    <ul>
      <li>network within a network</li>
      <li>ë™ì¼í•œ inputì„ ë°›ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì–‘í•œ í•„í„°ë“¤ì´ ë³‘ë ¬ë¡œ ì¡´ì¬</li>
      <li>ê° ê³„ì‚°ì„ ê±°ì¹œ í›„, concatenate</li>
      <li>ê³„ì‚° ë¹„ìš© ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ inception module ë‚´ì˜ ë³‘ë ¬ì ì¸ layerë“¤ì— ê°ê° 1x1 convì ìš©</li>
    </ul>
  </li>
  <li>auxiliary classifier ì¡´ì¬
    <ul>
      <li>ì‘ì€ ë¶€ê°€ ë„¤íŠ¸ì›Œí¬</li>
      <li>ImageNet trainset loss ê³„ì‚°í•¨ â†’ ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šê¸° ë•Œë¬¸ â†’ ì¶”ê°€ ì‹œ ì¤‘ê°„ ë ˆì´ì–´ì˜ í•™ìŠµì„ ë„ìš¸ ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/71377968/165305157-1711daff-e7b1-4444-9f12-8b59b81d1788.png" alt="image" /></p>

<p><br /></p>

<p>ResNet</p>

<ul>
  <li>ì•„ì£¼ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ (152)</li>
  <li>ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ”ê°€ ì§ˆë¬¸ â†’ ì„±ëŠ¥ì´ ë” ì•ˆ ì¢‹ì„ ìˆ˜ ìˆìŒ(optimization ë¬¸ì œ) â†’ networkê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ optimizationì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ</li>
  <li>Residual block ìŒ“ì•„ ì˜¬ë¦¼
    <ul>
      <li>H(x) - xë¼ëŠ” ë³€í™”ëŸ‰ì„ í•™ìŠµ (ì”ì°¨/residual)</li>
      <li>Skip connection ë„ì…</li>
      <li>ë‘ ê°œì˜ 3x3 conv layer</li>
    </ul>
  </li>
  <li>ì£¼ê¸°ì ìœ¼ë¡œ filterë¥¼ ë‘ë°° ì”© ëŠ˜ë¦¬ê³  stride 2ë¥¼ ì´ìš©í•´ downsampling</li>
  <li>ì´ˆë°˜ì—ëŠ” Conv layerê°€ ë¶™ê³  FC layerê°€ ì—†ìŒ. ëŒ€ì‹  Global Average Pooling ì§„í–‰</li>
  <li>ë§ˆì§€ë§‰ì—ëŠ” 1000ê°œì˜ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë…¸ë“œê°€ ë¶™ìŒ</li>
  <li>modelì˜ depthê°€ 50ì´ìƒì¼ ë•Œ bottleneck layerë¥¼ ë„ì…(depth ì¡°ì ˆ)</li>
  <li>Conv layer ë‹¤ìŒì— batch norm ì‚¬ìš©</li>
  <li>ì´ˆê¸°í™” xavier</li>
  <li>minibatch size 256, weight decay, no dropout, lr scheduling, 2ë¡œ ë‚˜ëˆ„ëŠ” scaling factor ì¶”ê°€(SGD + Momentumì—ì„œ ì´ˆê¸°í™” ì„±ëŠ¥)</li>
  <li>ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ train errorëŠ” ì¤„ì–´ë“¦</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/71377968/165305170-6bf1ae08-7cae-4a10-95ea-a4d676bc1ebb.png" alt="image" /></p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/71377968/165305191-f06d2f61-bbe1-452f-acae-bf196eb2092a.png" alt="image" /></p>

<p>ì•ì„œ ì‚´í´ë³¸ ëª¨ë¸ë“¤ì˜ model complexity ë¹„êµ</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/71377968/165305211-72a0f86c-2b21-4280-871a-19fb16503787.png" alt="image" /></p>

<p>forward pass timeê³¼ power consumption ë¹„êµ</p>

<p><br /></p>

<p>ë‹¤ë¥¸ architecture</p>

<ul>
  <li>Network in Network (NiN)
    <ul>
      <li>MLP conv layerë¥¼ ê¸°ë³¸ ì•„ì´ë””ì–´ë¡œ</li>
      <li>ê° conv ì•ˆì— MLP ìŒ“ìŒ</li>
      <li>ResNet, GoogLeNetë³´ë‹¤ ë¨¼ì € bottleneck ê°œë… ì •ë¦½</li>
    </ul>
  </li>
  <li>Identity Mappings in Deep Residual Networks
    <ul>
      <li>ResNet íŒŒìƒ</li>
      <li>ResNet block path ì¡°ì ˆ</li>
    </ul>
  </li>
  <li>Wide Residual Networks
    <ul>
      <li>depthë³´ë‹¤ residualì´ ì¤‘ìš”í•˜ë‹¤ê³  ì£¼ì¥</li>
      <li>Residual Connectionì´ ìˆë‹¤ë©´ ë„¤íŠ¸ì›Œí¬ê°€ ë” ê¹Šì–´ì§ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ì£¼ì¥</li>
      <li>residual blockì„ ë” ë„“ê²Œ ë§Œë“¦ â†’ filter í¬ê²Œ</li>
      <li>ë³‘ë ¬í™”ë˜ì–´ ê³„ì‚° íš¨ìœ¨ ì¦ê°€</li>
    </ul>
  </li>
  <li>ResNeXt
    <ul>
      <li>ResNetì˜ ì €ì</li>
      <li>width ê´€ë ¨ ì—°êµ¬</li>
      <li>residual block ë‚´ì— ë‹¤ì¤‘ ë³‘ë ¬ ê²½ë¡œë¥¼ ì¶”ê°€í•¨</li>
      <li>thinner blockì„ ë³‘ë ¬ë¡œ ì—¬ëŸ¬ê°œ ë¬¶ìŒ</li>
      <li>wWide ResNet, Inception Moduleê³¼ ì—°ê´€</li>
    </ul>
  </li>
  <li>Stochastic Depth
    <ul>
      <li>depthì— ê´€ë€ ì—°êµ¬</li>
      <li>train timeì— layerì˜ ì¼ë¶€ë¥¼ ì œê±° â†’ vanishing gradient problemì— ëŒ€í•œ sol</li>
      <li>dropoutê³¼ ìœ ì‚¬</li>
    </ul>
  </li>
  <li>FractalNet
    <ul>
      <li>non-ResNet</li>
      <li>shallow/deep networkì˜ ì •ë³´ ëª¨ë‘ë¥¼ ì˜ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ ìƒê°</li>
      <li>shallow/deep pathë¥¼ ëª¨ë‘ outputì— ì—°ê²°</li>
      <li>trainì—ëŠ” ì¼ë¶€, testì—ëŠ” full network ì‚¬ìš©</li>
    </ul>
  </li>
  <li>DenseNet
    <ul>
      <li>Dense Block</li>
      <li>í•œ layerê°€ ê·¸ í•˜ìœ„ì˜ ëª¨ë“  layerì™€ ì—°ê²°ë¨</li>
      <li>vanishing gradient problem ì™„í™”</li>
    </ul>
  </li>
  <li>SqueezeNet
    <ul>
      <li>íš¨ìœ¨ì„± ì¤‘ì‹œ</li>
      <li>fire module</li>
      <li>squeeze layer, expand layer</li>
    </ul>
  </li>
</ul>
:ET